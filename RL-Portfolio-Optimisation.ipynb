{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# Reinforcement Learning for Portfolio Optimisation\n", "\n", "This notebook demonstrates how to optimise a financial portfolio using both classical methods (Modern Portfolio Theory) and modern reinforcement learning (RL) techniques, including Proximal Policy Optimisation (PPO).\n", "\n", "It is intended as a portfolio project example for roles involving applied AI, machine learning in finance, and reinforcement learning in production environments.\n", "\n", "The following steps will be covered:\n", "\n", "1. Download and prepare financial data\n", "2. Classical optimisation using Markowitz's theory\n", "3. Construction of a custom RL environment\n", "4. Training an RL agent (PPO)\n", "5. Comparison of results\n", "6. Backtesting and evaluation\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Install required packages\n", "!pip install yfinance stable-baselines3 gym pandas matplotlib"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Step 1: Download Financial Data"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import yfinance as yf\n", "import numpy as np\n", "import pandas as pd\n", "import matplotlib.pyplot as plt\n", "\n", "# Define stock tickers and download historical prices\n", "tickers = ['AAPL', 'GOOGL', 'MSFT', 'AMZN', 'TSLA']\n", "data = yf.download(tickers, start=\"2020-01-01\", end=\"2022-01-01\")[\"Close\"]\n", "daily_returns = data.pct_change().dropna()\n", "expected_returns = daily_returns.mean()\n", "cov_matrix = daily_returns.cov()\n", "\n", "data.head()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Step 2: Classical Portfolio Optimisation\n", "\n", "Using Markowitz Modern Portfolio Theory to minimise risk (portfolio variance)."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from scipy.optimize import minimize\n", "\n", "def compute_portfolio_variance(weights, cov_matrix):\n", "    return np.dot(weights.T, np.dot(cov_matrix, weights))\n", "\n", "def constraint(weights):\n", "    return np.sum(weights) - 1\n", "\n", "bounds = [(0, 1)] * len(tickers)\n", "initial_weights = np.random.dirichlet(np.ones(len(tickers)), size=1).flatten()\n", "\n", "constraints = [\n", "    {'type': 'eq', 'fun': constraint},\n", "    {'type': 'ineq', 'fun': lambda w: w - 0.05}\n", "]\n", "\n", "result = minimize(compute_portfolio_variance, initial_weights, args=(cov_matrix,),\n", "                  method='SLSQP', bounds=bounds, constraints=constraints)\n", "\n", "optimized_weights = result.x\n", "portfolio_return = np.dot(expected_returns, optimized_weights)\n", "portfolio_variance_value = compute_portfolio_variance(optimized_weights, cov_matrix)\n", "\n", "print(\"Optimised Weights:\", optimized_weights)\n", "print(\"Expected Return:\", portfolio_return)\n", "print(\"Portfolio Variance:\", portfolio_variance_value)\n", "\n", "# Plot\n", "plt.bar(tickers, optimized_weights)\n", "plt.title(\"Optimised Portfolio Weights (Classical)\")\n", "plt.xlabel(\"Stock\")\n", "plt.ylabel(\"Weight\")\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Step 3: Create a Custom RL Environment\n", "This environment simulates portfolio allocation over time."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import gym\n", "from gym import spaces\n", "\n", "class PortfolioEnv(gym.Env):\n", "    def __init__(self, returns, initial_balance=1000):\n", "        super(PortfolioEnv, self).__init__()\n", "        self.returns = returns.values\n", "        self.n_assets = self.returns.shape[1]\n", "        self.initial_balance = initial_balance\n", "        self.max_steps = len(returns) - 1\n", "\n", "        self.action_space = spaces.Box(low=0.05, high=1.0, shape=(self.n_assets,), dtype=np.float32)\n", "        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(self.n_assets,), dtype=np.float32)\n", "\n", "    def reset(self):\n", "        self.current_step = 0\n", "        self.weights = np.ones(self.n_assets) / self.n_assets\n", "        self.nav = self.initial_balance\n", "        return self.returns[self.current_step]\n", "\n", "    def step(self, action):\n", "        action = np.clip(action, 0.05, 1)\n", "        action = action / np.sum(action)\n", "        self.current_step += 1\n", "\n", "        prev_returns = self.returns[self.current_step - 1]\n", "        next_returns = self.returns[self.current_step]\n", "        self.nav *= (1 + np.dot(prev_returns, self.weights))\n", "        self.weights = action\n", "\n", "        reward = np.dot(next_returns, self.weights)\n", "        done = self.current_step >= self.max_steps - 1\n", "        obs = self.returns[self.current_step]\n", "        return obs, reward, done, {}"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Step 4: Train PPO Agent on Portfolio Environment"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from stable_baselines3 import PPO\n", "from stable_baselines3.common.env_checker import check_env\n", "\n", "env = PortfolioEnv(daily_returns)\n", "check_env(env)\n", "\n", "model = PPO(\"MlpPolicy\", env, verbose=0)\n", "model.learn(total_timesteps=50000)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Step 5: Evaluate PPO Agent"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["obs = env.reset()\n", "rewards = []\n", "\n", "for _ in range(env.max_steps):\n", "    action, _ = model.predict(obs)\n", "    obs, reward, done, _ = env.step(action)\n", "    rewards.append(reward)\n", "    if done:\n", "        break\n", "\n", "print(f\"Final NAV: \u00a3{env.nav:.2f}\")\n", "print(f\"Mean Daily Return (RL): {np.mean(rewards):.4f}\")\n", "\n", "# Plot performance\n", "plt.plot(np.cumprod(1 + np.array(rewards)), label=\"PPO Agent\")\n", "plt.axhline(y=(1 + portfolio_return) ** len(rewards), color='r', linestyle='--', label=\"Classical\")\n", "plt.title(\"Cumulative Return: PPO vs Classical\")\n", "plt.xlabel(\"Time Step\")\n", "plt.ylabel(\"Portfolio Growth\")\n", "plt.legend()\n", "plt.show()"]}], "metadata": {"colab": {"name": "RL_Portfolio_Optimisation.ipynb"}, "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"name": "python", "version": "3.8"}}, "nbformat": 4, "nbformat_minor": 5}